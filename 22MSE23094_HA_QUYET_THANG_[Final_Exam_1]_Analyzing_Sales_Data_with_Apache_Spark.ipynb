{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Analyzing_Sales_Data_with_Apache_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SUBJECT: BIG DATA </br>\n",
        "CLASS : MSE12#HCM <br>\n",
        "MSHV : 22MSE23094 </br>\n",
        "MSE STUDENT's NAME : HA QUYET THANG </br>\n",
        "INSTRUCTOR: DR. MAI HOANG BAO AN"
      ],
      "metadata": {
        "id": "6a5c8OX_lbZW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Term Exam 1 - Analyzing Sales Data with Apache Spark\n",
        "\n",
        "## Problem Statement\n",
        "In this project, you will use Apache Spark to analyze a large dataset containing\n",
        "sales data posted here. The dataset includes information such as Sample Sales Data, Order Info, Sales, Customer, Shipping, etc...\n",
        "\n",
        "Data get from kaggle and và upload to persional kaggle :\n",
        "[Dataset](https://www.kaggle.com/datasets/thanghq/sales-data-sample)\n",
        "\n",
        "+ Your goal is to perform various analyses on this dataset to derive valuable insights for the company."
      ],
      "metadata": {
        "id": "afLIVNMoYcjx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup environment"
      ],
      "metadata": {
        "id": "UDOwMb38Zn_b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQW0uxbm9Ydl"
      },
      "outputs": [],
      "source": [
        "# Update package information\n",
        "!apt-get update\n",
        "\n",
        "# Install OpenJDK 8 (Java Development Kit) headless version\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Download Apache Spark 3.1.1 binary with Hadoop 3.2\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "\n",
        "# Extract the downloaded Spark archive\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "\n",
        "# Install findspark library to help Python locate Spark\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set environment variables for Java and Spark home directories\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "\n",
        "# Initialize findspark to enable locating Spark in Python\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the Kaggle library\n",
        "!pip install -q kaggle\n",
        "\n",
        "# # Import necessary modules for file upload\n",
        "# from google.colab import files\n",
        "\n",
        "# # Upload the Kaggle API key file\n",
        "# files.upload()\n",
        "\n",
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "metadata": {
        "id": "Dt6nBxRcaN7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cấu hình secret key để xác thực truy cập đến Kaggle trên môi trường\n",
        "os.environ['KAGGLE_USERNAME'] = 'thanghq'\n",
        "os.environ['KAGGLE_KEY'] = '5a9cdf308c14b0866956569d327d2e82'\n",
        "\n",
        "if not os.path.exists('./kaggle/sales_data_sample.csv'):\n",
        "  !kaggle datasets download -d thanghq/sales-data-sample -p ./ # Download dữ liệu từ Kaggle\n",
        "  !unzip  ./*.zip -d ./kaggle\n",
        "else:\n",
        "    print('File already exists, no need to download or unzip.')\n",
        "\n",
        "print(os.listdir('./kaggle'))"
      ],
      "metadata": {
        "id": "iyAz3lyMfDG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data Preparation"
      ],
      "metadata": {
        "id": "sNBPTJnYYyeS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the sales dataset into Spark RDDs or DataFrames."
      ],
      "metadata": {
        "id": "jOpSa5uZY0Yx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "c9nvCjnl9cYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc = SparkContext(conf=SparkConf())\n",
        "spark = SparkSession(sparkContext=sc)"
      ],
      "metadata": {
        "id": "VgcxKPhkoYle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "print('Pandas version: {}'. format(pd.__version__))"
      ],
      "metadata": {
        "id": "k9eLCVyNAGc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_raw = spark.read.csv('/content/kaggle/sales_data_sample.csv', inferSchema=True, header=True)\n",
        "\n",
        "# preview the data\n",
        "# data type\n",
        "print('-'*10, 'data types', '-'*10)\n",
        "pd.DataFrame(data_raw.dtypes)"
      ],
      "metadata": {
        "id": "vhCf_2vh_4Rh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert Spark DataFrame to Pandas DataFrame\n",
        "raw_df = data_raw.toPandas()\n",
        "\n",
        "# Create a copy of the Pandas DataFrame\n",
        "df = raw_df.copy()\n",
        "\n",
        "df_org = df\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "df.head()"
      ],
      "metadata": {
        "id": "MMIIv9YZBjgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data summary\n",
        "print('-'*10, 'data summary', '-'*10)\n",
        "data_raw.describe().toPandas()"
      ],
      "metadata": {
        "id": "Dx5Y0EltANsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# view a small subset of the data\n",
        "print('-'*10, 'randomely sample 1% data to view', '-'*10)\n",
        "data_raw.randomSplit([0.01, 0.99])[0].toPandas()"
      ],
      "metadata": {
        "id": "YX7DXNv8ARco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "iOH3asOxAUra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "uSrd_wbDBNMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cleanse the data by handling missing values, outliers, or any inconsistencies."
      ],
      "metadata": {
        "id": "uO9pEXnzZBtT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Checking for null values"
      ],
      "metadata": {
        "id": "zTsEWbIqBRLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display information about the DataFrame\n",
        "df.info()"
      ],
      "metadata": {
        "id": "m5B9BvJcskdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check NUll Values\n",
        "isnull=pd.DataFrame(df.isnull().sum())\n",
        "isnull.style.background_gradient(cmap='Blues')"
      ],
      "metadata": {
        "id": "_ZtEVHzphC2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the total number of missing values for each column\n",
        "total_null = df.isnull().sum().sort_values(ascending=False)\n",
        "\n",
        "# Calculate the percentage of missing values for each column\n",
        "percent_null = (df.isnull().sum() / df.isnull().count()).sort_values(ascending=False)\n",
        "\n",
        "# Combine the total_null and percent_null into a single DataFrame\n",
        "missing_data = pd.concat([total_null, percent_null], axis=1, keys=['total_null', 'percent_null'])\n",
        "\n",
        "# Display the DataFrame showing total_null and percent_null for each column\n",
        "missing_data"
      ],
      "metadata": {
        "id": "A2xkBwaOBSVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing the variables which dont add significant value to the analysis or majority null value.\n",
        "to_drop = ['PHONE','ADDRESSLINE1','ADDRESSLINE2','STATE','POSTALCODE','TERRITORY']\n",
        "df = df.drop(to_drop, axis=1)"
      ],
      "metadata": {
        "id": "1YOw0A8eBtrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "2unJTqqRgBSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Checking for inconsistent data types"
      ],
      "metadata": {
        "id": "6pnR3grHB0bA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the data types of each column in the DataFrame\n",
        "print(df.dtypes)"
      ],
      "metadata": {
        "id": "kzRJ2XoBBUt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['ORDERDATE'] = pd.to_datetime(df['ORDERDATE'])"
      ],
      "metadata": {
        "id": "VT9nogQ7B2co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Summary stats of Quantitative variables"
      ],
      "metadata": {
        "id": "GDdI-D00B5X4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quant_vars = ['QUANTITYORDERED','PRICEEACH','SALES','MSRP']\n",
        "df[quant_vars].describe()"
      ],
      "metadata": {
        "id": "uXemH6fFB4RI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.sort_values(by = ['ORDERDATE'], inplace = True)\n",
        "df.set_index('ORDERDATE', inplace = True)"
      ],
      "metadata": {
        "id": "pthVirDyEr1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "6qfe4Z-TDldI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Outlier detection\n",
        "https://www.kaggle.com/code/shohanurrahaman/intro-to-data-science-data-cleaning-02\n",
        "\n",
        "Outlier detection is very important step in data cleaning and exploring. Outliers can be detected both visually and mathematically. Some plots are very helpful in visualizing outliers, such as box plots and scatter plots. However, it is sometimes tricky to decide whether or not to remove the outliers. We should remove outliers when we are certain that these outliers were results of some errors."
      ],
      "metadata": {
        "id": "r4cGoPgv0g9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Filtering outlier"
      ],
      "metadata": {
        "id": "GoCdSakt0wYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('original shape of dataset :',df.shape)\n",
        "\n",
        "cols = ['SALES', 'MSRP','QUANTITYORDERED']\n",
        "new_df = df[cols]\n",
        "\n",
        "#calculation\n",
        "Q1 = new_df.quantile(0.25)\n",
        "Q3 = new_df.quantile(0.75)\n",
        "IQR = Q3-Q1\n",
        "maximum = Q3+1.5*IQR\n",
        "minimum = Q1-1.5*IQR\n",
        "print(minimum)\n",
        "\n",
        "#filter outlier\n",
        "cond = (new_df <= maximum) & (new_df >= minimum)\n",
        "'''\n",
        "we specify that the condition should be true for all three columns by using the all function with axis=1 argument.\n",
        "This gives us a list of True/False against each row.\n",
        "If a row has all three True values, then it gives a True value to that row\n",
        "'''\n",
        "cond = cond.all(axis=1)\n",
        "df = df[cond]\n",
        "print('filtered dataset shape : ',df.shape)\n",
        "\n",
        "#plot again to check that if has any outlier\n",
        "df.plot(kind='box', subplots=True, sharex=False, sharey=False, figsize=(10,10), layout=(3,4))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "d6zqHpsg0298"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Scatter Plot"
      ],
      "metadata": {
        "id": "oqu5VD1n07-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_df = df[['SALES','QUANTITYORDERED','MSRP']]\n",
        "pd.plotting.scatter_matrix(new_df, figsize = (10,10))"
      ],
      "metadata": {
        "id": "ky0nqzVN1EFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Z-Score"
      ],
      "metadata": {
        "id": "r5LQFfXd1O_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('shape of original data :',df.shape)\n",
        "\n",
        "mean = new_df['QUANTITYORDERED'].mean()\n",
        "std_dev = new_df['QUANTITYORDERED'].std()\n",
        "\n",
        "# find z scores\n",
        "z_scores = (new_df['QUANTITYORDERED'] - mean) / std_dev\n",
        "z_scores = np.abs(z_scores)\n",
        "\n",
        "print(z_scores.min())\n",
        "\n",
        "#filter data\n",
        "z_df = new_df[z_scores<3]\n",
        "print('shape of filtered data : ',z_df.shape)\n",
        "\n",
        "#plot data\n",
        "z_df['QUANTITYORDERED'].plot(kind='box')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "T9Vsv1aK1Qtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. EDA - Exploratory Data Analysis and Visualization"
      ],
      "metadata": {
        "id": "3FRVdl12E7Qw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Perform descriptive statistics to understand the basic characteristics of the dataset and visualize."
      ],
      "metadata": {
        "id": "uX88ujwSqjMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe().transpose()"
      ],
      "metadata": {
        "id": "kbh_2unCrG0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ys=df.groupby(['YEAR_ID'])['SALES'].sum().reset_index()\n",
        "ys.head()"
      ],
      "metadata": {
        "id": "I33nJ-2n7z9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group the DataFrame by 'PRODUCTLINE' and 'QTR_ID', count occurrences, and reset index\n",
        "qtrly = df.groupby(['PRODUCTLINE']).QTR_ID.value_counts().reset_index(name='COUNTS')\n",
        "qtrly.head()\n",
        "\n",
        "# Create a figure with a specific size\n",
        "plt.figure(figsize=(10,8))\n",
        "\n",
        "# Define colors for the strip plot\n",
        "colors={'edgecolor':'black','linewidth':1}\n",
        "\n",
        "# Create a strip plot using Seaborn\n",
        "sns.stripplot(x='PRODUCTLINE', y='COUNTS', data=qtrly, hue='QTR_ID', palette='bright', size=14, **colors)\n",
        "\n",
        "# Set the background style of the plot to 'whitegrid'\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "# Set the title of the plot\n",
        "plt.title(\"PRODUCTS SOLD ACCORDING TO QUARTER\")\n",
        "\n",
        "# Set labels for the x-axis and y-axis\n",
        "plt.xlabel('PRODUCTS')\n",
        "plt.ylabel('NUMBER OF ITEMS SOLD')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cGrynXsk9zrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explore the distribution of sales across different products, customers, and time periods and visualize.\n"
      ],
      "metadata": {
        "id": "EQe6ewtaFGhf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This plot visualizes the relationship between the product line and sales."
      ],
      "metadata": {
        "id": "6xe-yf70I7oc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a figure with a specific size for the correlation matrix heatmap\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "# Calculate the correlation matrix for the DataFrame\n",
        "corr_matrix = df.corr()\n",
        "\n",
        "# Create a heatmap with annotations using Seaborn\n",
        "sns.heatmap(corr_matrix, annot=True)\n",
        "\n",
        "# Set the title of the heatmap\n",
        "plt.title(\"Correlation Matrix\")\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aT0ZG2MrFR5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a bar plot to visualize sales across different product lines\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Set the layout of the plot\n",
        "plt.tight_layout()\n",
        "\n",
        "# Use Seaborn to create a bar plot\n",
        "sns.barplot(x='PRODUCTLINE', y='SALES', data=df)\n",
        "\n",
        "# Set the title of the plot\n",
        "plt.title(\"Sales Across Different Product Lines\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0XivoQu9Zipa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The x-axis represents the unique values of 'STATUS', and the y-axis represents the count of occurrences"
      ],
      "metadata": {
        "id": "-xUcBBQmJFLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a count plot to visualize the distribution of 'STATUS' values\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Use Seaborn to create a count plot\n",
        "sns.countplot(df['STATUS'])\n",
        "\n",
        "# Set the title of the plot\n",
        "plt.title(\"Distribution of Status Values\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "grAhjkdZZmmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a line plot to visualize the sales trend over time\n",
        "plt.figure(figsize=(20, 8))\n",
        "\n",
        "# Use Seaborn to create a line plot\n",
        "sns.lineplot(x='ORDERDATE', y='SALES', data=df)\n",
        "\n",
        "# Set the title of the plot\n",
        "plt.title(\"Sales Trend Over Time\")\n",
        "\n",
        "# Set labels for the x and y axes\n",
        "plt.xlabel('Order Date')\n",
        "plt.ylabel('Sales')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kPDQ4rq_XVtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then visualized which products give the highest revenue"
      ],
      "metadata": {
        "id": "pcbO6kIDIuIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by 'PRODUCTLINE', sum the 'SALES', and sort in descending order\n",
        "top_product = df.groupby(['PRODUCTLINE']).sum().sort_values('SALES', ascending=False)\n",
        "\n",
        "# Select only the 'SALES' column\n",
        "top_product = top_product[['SALES']]\n",
        "\n",
        "# Reset the index to make 'PRODUCTLINE' a regular column\n",
        "top_product.reset_index(inplace=True)\n",
        "\n",
        "# Calculate the total revenue from all products\n",
        "total_revenue_product = top_product['SALES'].sum()\n",
        "\n",
        "# Format the total revenue as a string with '$' symbol\n",
        "total_revenue_product = f'${int(total_revenue_product):,}'"
      ],
      "metadata": {
        "id": "SxeB1eKeIwIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set default figure size and font parameters\n",
        "plt.rcParams['figure.figsize'] = (13, 7)\n",
        "plt.rcParams['font.size'] = 12.0\n",
        "plt.rcParams['font.weight'] = 6\n",
        "\n",
        "# Define a function for autopct formatting\n",
        "def autopct_format(values):\n",
        "    def my_format(pct):\n",
        "        total = sum(values)\n",
        "        val = int(round(pct * total / 100.0))\n",
        "        return ' ${v:d}'.format(v=val)\n",
        "    return my_format\n",
        "\n",
        "# Define colors, explode, and create subplots\n",
        "colors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99', '#55B4B0', '#E15D44', '#009B77']\n",
        "explode = (0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05)\n",
        "fig1, ax1 = plt.subplots()\n",
        "\n",
        "# Create a pie chart with custom formatting\n",
        "pie1 = ax1.pie(top_product['SALES'], colors=colors, labels=top_product['PRODUCTLINE'],\n",
        "               autopct=autopct_format(top_product['SALES']), startangle=90, explode=explode)\n",
        "\n",
        "# Rotate fraction text in the pie chart\n",
        "fraction_text_list = pie1[2]\n",
        "for text in fraction_text_list:\n",
        "    text.set_rotation(315)\n",
        "\n",
        "# Create a white circle in the center to make it a donut chart\n",
        "center_circle = plt.Circle((0, 0), 0.80, fc='white')\n",
        "fig = plt.gcf()\n",
        "fig.gca().add_artist(center_circle)\n",
        "\n",
        "# Set axis equal, add annotation for total revenue, and display the plot\n",
        "ax1.axis('equal')\n",
        "label = ax1.annotate('Total Revenue \\n' + str(total_revenue_product), color='red', xy=(0, 0), fontsize=12, ha='center')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jgVBWRH_In6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Feature Engineering\n",
        "Create additional features from the existing data that might be useful for analysis, such as calculating total sales per customer, average purchase amount per product, or seasonal trends."
      ],
      "metadata": {
        "id": "jnUiI0-UZ1Ca"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Find out 20 Most Valuable Customers\n",
        "\n",
        "The Most Valuable Customers are the customer who are the most profitable for a company (have a big sales on them). These customers buy more or higher-value than the other customers."
      ],
      "metadata": {
        "id": "9pJsiv2GH5zF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by 'CUSTOMERNAME', sum the 'SALES', and sort in descending order, then select top 20\n",
        "top_customer = df.groupby(['CUSTOMERNAME']).sum().sort_values('SALES', ascending=False).head(20)\n",
        "\n",
        "# Select only the 'SALES' column and round values to 3 decimal places\n",
        "top_customer = top_customer[['SALES']].round(3)\n",
        "\n",
        "# Reset the index to make 'CUSTOMERNAME' a regular column\n",
        "top_customer.reset_index(inplace=True)"
      ],
      "metadata": {
        "id": "BndkocB6BtVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up plot parameters\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.title('20 Most Valuable Customers (2003 - 2005)', fontsize=18)\n",
        "\n",
        "# Create a bar plot\n",
        "plt.bar(top_customer['CUSTOMERNAME'], top_customer['SALES'], color='#37C6AB', edgecolor='black', linewidth=1)\n",
        "\n",
        "# Set labels for x-axis and y-axis\n",
        "plt.xlabel('Customer Name', fontsize=15)\n",
        "plt.ylabel('Revenue', fontsize=15)\n",
        "\n",
        "# Customize tick labels and rotation for better readability\n",
        "plt.xticks(fontsize=12, rotation=90)\n",
        "plt.yticks(fontsize=12)\n",
        "\n",
        "# Add revenue values as text on each bar with conditional formatting\n",
        "for k, v in top_customer['SALES'].items():\n",
        "    if v > 600000:\n",
        "        plt.text(k, v - 270000, '$' + str(v), fontsize=12, rotation=90, color='black', ha='center')\n",
        "    else:\n",
        "        plt.text(k, v + 50000, '$' + str(v), fontsize=12, rotation=90, color='black', ha='center')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MHhUVEarHz_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Find out 20 Highest Revenue by Country\n",
        "Here are The Top 20 Country which generated the highest revenue"
      ],
      "metadata": {
        "id": "5p25ZDMqID60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by 'COUNTRY', sum the 'SALES', and sort in descending order, then select top 20\n",
        "top_country = df.groupby(['COUNTRY']).sum().sort_values('SALES', ascending=False).head(20)\n",
        "\n",
        "# Select only the 'SALES' column and round values to 3 decimal places\n",
        "top_country = top_country[['SALES']].round(3)\n",
        "\n",
        "# Reset the index to make 'COUNTRY' a regular column\n",
        "top_country.reset_index(inplace=True)"
      ],
      "metadata": {
        "id": "e_5ZeAA8B4Fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up plot parameters\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.title('20 Highest Revenue by Country (2003 - 2005)', fontsize=18)\n",
        "\n",
        "# Create a bar plot\n",
        "plt.bar(top_country['COUNTRY'], top_country['SALES'], color='#37C6AB', edgecolor='black', linewidth=1)\n",
        "\n",
        "# Set labels for x-axis and y-axis\n",
        "plt.xlabel('Country', fontsize=15)\n",
        "plt.ylabel('Revenue', fontsize=15)\n",
        "\n",
        "# Customize tick labels and rotation for better readability\n",
        "plt.xticks(fontsize=12, rotation=90)\n",
        "plt.yticks(fontsize=12)\n",
        "\n",
        "# Add revenue values as text on each bar with conditional formatting\n",
        "for k, v in top_country['SALES'].items():\n",
        "    if v > 3000000:\n",
        "        plt.text(k, v - 1200000, '$' + str(v), fontsize=12, rotation=90, color='black', ha='center')\n",
        "    else:\n",
        "        plt.text(k, v + 100000, '$' + str(v), fontsize=12, rotation=90, color='black', ha='center')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UWKTPHcjIKbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Correlation Test\n",
        "Plotting correlation matrix to see the overview of how the features are related to one another"
      ],
      "metadata": {
        "id": "nDSk4tX2LvbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up plot parameters\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "# Calculate correlation matrix\n",
        "corr_matrix = df.corr()"
      ],
      "metadata": {
        "id": "LNHmLMZMMUv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observations\n",
        "\n",
        "There is high co-relation in ORDERNUMBER and YEAR_ID, and between QTR_ID and MONTH_ID\n",
        "+velly correlated between SALES, QUANTITYORDERED, PRICEEACH and MSRP\n",
        "YEAR_ID is -velly correlated to QTR_ID and MONTH_ID"
      ],
      "metadata": {
        "id": "_E9RTD7hMGBj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Advanced Analytics"
      ],
      "metadata": {
        "id": "iNijCpatexUX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Time Series Analysis"
      ],
      "metadata": {
        "id": "6DFRNgPpMtbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Order Date Description\\n')\n",
        "\n",
        "# Sort the DataFrame by 'ORDERDATE' in ascending order\n",
        "df.sort_values(by=['ORDERDATE'], inplace=True, ascending=True)\n",
        "\n",
        "# Create a new DataFrame with only 'SALES' column\n",
        "new_data = pd.DataFrame(df['SALES'])\n",
        "new_data.head()"
      ],
      "metadata": {
        "id": "nmPeglRre2Eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Plot a histogram of the 'SALES' column with specified parameters\n",
        "new_data['SALES'].plot(kind='hist', bins=20, title='SALES')\n",
        "\n",
        "# Hide the spines on the top and right sides of the plot\n",
        "plt.gca().spines[['top', 'right']].set_visible(False)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "shaEut9QNURB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_data.plot()"
      ],
      "metadata": {
        "id": "cnsNK12bNk7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A series is said to be stationary when its mean and variance do not change over time. From the above distribution of the sales it is not clear whether the sales distribution is stationary or not. Let us perform some stationarity tests to check whether the time series is stationary or not."
      ],
      "metadata": {
        "id": "YWALddOCfcWG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking for Stationary"
      ],
      "metadata": {
        "id": "9qh5CHvEferH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Resample the 'SALES' column by day and calculate the mean for each day\n",
        "new_data = pd.DataFrame(new_data['SALES'].resample('D').mean())\n",
        "\n",
        "# Interpolate missing values using linear interpolation\n",
        "new_data = new_data.interpolate(method='linear')"
      ],
      "metadata": {
        "id": "b1-4w4tlfh3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Method 1<br/>\n",
        "\n",
        "To check for stationarity by comparing the change in mean and variance over time, let us split teh data into train, test, and validation"
      ],
      "metadata": {
        "id": "oy9Pqdb4flB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the 'SALES' column into train, test, and validation sets\n",
        "train, test, validation = np.split(new_data['SALES'].sample(frac=1), [int(0.6 * len(new_data['SALES'])), int(0.8 * len(new_data['SALES']))])"
      ],
      "metadata": {
        "id": "A_7_OCDPfbzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train Dataset')\n",
        "print(train)\n",
        "print('Test Dataset')\n",
        "print(test)\n",
        "print('Validation Dataset')\n",
        "print(validation)"
      ],
      "metadata": {
        "id": "0SmYcMh1fY-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above values of mean and variance, it can be inferred that their is not much difference in the three values of mean and variance, indicating that the series is stationary. However, to verify our observations, let us perform a standard stationarity test, called Augmented Dicky Fuller test.\n",
        "\n",
        "Augmented Dicky Fuller Test\n",
        "\n",
        "- The Augmented Dickey-Fuller test is a type of statistical test alsocalled a unit root test.The base of unit root test is that it helps in determining how strongly a time series is defined by a trend.\n",
        "- The null hypothesis of the test is that the time series can be represented by a unit root, that it is not stationary. The alternate hypothesis (rejecting the null hypothesis) is that the time series is stationary.\n",
        "  - Null Hypothesis(H0): Time series is not stationary\n",
        "  - Alternate Hypothesis (H1): Time series is stationary\n",
        "- This result is interpreted using the p-value from the test.\n",
        "  - p-value > 0.05: Fail to reject the null hypothesis (H0), the data has a unit root and is non-stationary.\n",
        "  - p-value <= 0.05: Reject the null hypothesis (H0), the data does not have a unit root and is stationary."
      ],
      "metadata": {
        "id": "0YQzeX53fxEH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Method 2 - Augmented Dicky Fuller Test"
      ],
      "metadata": {
        "id": "zOAG_hK3gM4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "# Extract the 'SALES' column values\n",
        "data1 = new_data.iloc[:, 0].values\n",
        "\n",
        "# Perform Augmented Dickey-Fuller test\n",
        "adf = adfuller(data1)\n",
        "\n",
        "print(adf)\n",
        "print('\\nADF = ', str(adf[0]))\n",
        "print('\\np-value = ', str(adf[1]))\n",
        "print('\\nCritical Values: ')\n",
        "\n",
        "# Print critical values and interpret the results\n",
        "for key, val in adf[4].items():\n",
        "    print(key, ':', val)\n",
        "    if adf[0] < val:\n",
        "        print('Null Hypothesis Rejected. Time Series is Stationary')\n",
        "    else:\n",
        "        print('Null Hypothesis Accepted. Time Series is not Stationary')"
      ],
      "metadata": {
        "id": "duY7ShUPfrgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pylab import rcParams\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set the figure size\n",
        "rcParams['figure.figsize'] = 20, 10\n",
        "\n",
        "# Seasonal decomposition using the additive model\n",
        "decomposition = sm.tsa.seasonal_decompose(new_data, model='additive')\n",
        "\n",
        "# Plot the decomposition components\n",
        "fig = decomposition.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lQHyYx7ggQyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking for Stationary"
      ],
      "metadata": {
        "id": "b8WtNsXhOaxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Resample the 'SALES' column to daily frequency and calculate the mean for each day\n",
        "new_data = pd.DataFrame(new_data['SALES'].resample('D').mean())\n",
        "\n",
        "# Interpolate missing values using linear interpolation\n",
        "new_data = new_data.interpolate(method='linear')"
      ],
      "metadata": {
        "id": "ZAm11fANOgOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Method 1\n",
        "To check for stationarity by comparing the change in mean and variance over time, let us split teh data into train, test, and validation"
      ],
      "metadata": {
        "id": "jEt8fNLDOlMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train, test, validation = np.split(new_data['SALES'].sample(frac = 1), [int(.6*len(new_data['SALES'])), int(.8*len(new_data['SALES']))])"
      ],
      "metadata": {
        "id": "srk3R4h4OqCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train Dataset')\n",
        "print(train)\n",
        "print('Test Dataset')\n",
        "print(test)\n",
        "print('Validation Dataset')\n",
        "print(validation)"
      ],
      "metadata": {
        "id": "jmNyugnIPTMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above values of mean and variance, it can be inferred that their is not much difference in the three values of mean and variance, indicating that the series is stationary. However, to verify our observations, let us perform a standard stationarity test, called Augmented Dicky Fuller test."
      ],
      "metadata": {
        "id": "vn18RUARO12q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Method 2 - Augmented Dicky Fuller Test"
      ],
      "metadata": {
        "id": "keTSTuUiPbd5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "# Extract the 'SALES' values from the DataFrame\n",
        "data1 = new_data.iloc[:, 0].values\n",
        "\n",
        "# Perform the Augmented Dickey-Fuller test\n",
        "adf = adfuller(data1)\n",
        "\n",
        "# Print the test results\n",
        "print(adf)\n",
        "print('\\nADF = ', str(adf[0]))\n",
        "print('\\np-value = ', str(adf[1]))\n",
        "print('\\nCritical Values: ')\n",
        "\n",
        "# Loop through the critical values\n",
        "for key, val in adf[4].items():\n",
        "    print(key, ':', val)\n",
        "    # Check if the test statistic is less than the critical value\n",
        "    if adf[0] < val:\n",
        "        print('Null Hypothesis Rejected. Time Series is Stationary')\n",
        "    else:\n",
        "        print('Null Hypothesis Accepted. Time Series is not Stationary')"
      ],
      "metadata": {
        "id": "heOUP4TmPerl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pylab import rcParams\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Set the figure size\n",
        "rcParams['figure.figsize'] = 20, 10\n",
        "\n",
        "# Perform seasonal decomposition\n",
        "decomposition = sm.tsa.seasonal_decompose(new_data, model='additive')\n",
        "\n",
        "# Plot the decomposition components\n",
        "fig = decomposition.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lavFVLt8PimY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sales Forecasting using ARIMA\n",
        "Now that we know our time series is data is stationary. Let us begin with model training for forecasting the sales. We have chosen SARIMA model to forecast the sales.\n",
        "\n",
        "Seasonal Autoregressive Integrated Moving Average, SARIMA or Seasonal ARIMA, is an extension of ARIMA that supports univariate time series data with a seasonal componen"
      ],
      "metadata": {
        "id": "45x5EjoaPxW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "# Generate parameter combinations for Seasonal ARIMA\n",
        "p = d = q = range(0, 2)\n",
        "pdq = list(itertools.product(p, d, q))\n",
        "seasonal_pdq_comb = [(i[0], i[1], i[2], 12) for i in list(itertools.product(p, d, q))]\n",
        "\n",
        "# Display examples of parameter combinations\n",
        "print('Examples of parameter combinations for Seasonal ARIMA:')\n",
        "print('SARIMA: {} x {}'.format(pdq[1], seasonal_pdq_comb[1]))\n",
        "print('SARIMA: {} x {}'.format(pdq[1], seasonal_pdq_comb[2]))\n",
        "print('SARIMA: {} x {}'.format(pdq[2], seasonal_pdq_comb[3]))\n",
        "print('SARIMA: {} x {}'.format(pdq[2], seasonal_pdq_comb[4]))"
      ],
      "metadata": {
        "id": "iLqPSwHKQCLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "# Generate parameter combinations for Seasonal ARIMA\n",
        "p = d = q = range(0, 2)\n",
        "pdq = list(itertools.product(p, d, q))\n",
        "seasonal_pdq_comb = [(i[0], i[1], i[2], 12) for i in list(itertools.product(p, d, q))]\n",
        "\n",
        "# Iterate over parameter combinations\n",
        "for parameters in pdq:\n",
        "    for seasonal_param in seasonal_pdq_comb:\n",
        "        try:\n",
        "            # Fit SARIMA model\n",
        "            mod = sm.tsa.statespace.SARIMAX(new_data,\n",
        "                                            order=parameters,\n",
        "                                            seasonal_order=seasonal_param,\n",
        "                                            enforce_stationarity=False,\n",
        "                                            enforce_invertibility=False)\n",
        "            results = mod.fit()\n",
        "\n",
        "            # Print AIC\n",
        "            print('SARIMA{}x{}12 - AIC:{}'.format(parameters, seasonal_param, results.aic))\n",
        "        except Exception as e:\n",
        "            print(f\"Error for SARIMA{parameters}x{seasonal_param}12: {e}\")\n",
        "            continue"
      ],
      "metadata": {
        "id": "pMjKdY0JQHBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mod = sm.tsa.statespace.SARIMAX(new_data,\n",
        "                                order=(1, 1, 1),\n",
        "                                seasonal_order=(1, 1, 1, 12),\n",
        "                                enforce_stationarity=False,\n",
        "                                enforce_invertibility=False)\n",
        "results = mod.fit()\n",
        "print(results.summary().tables[1])"
      ],
      "metadata": {
        "id": "DWYTBuvvRH1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code fits a Seasonal ARIMA model with specified parameters to the new_data time series and prints a summary of the results, specifically the second table of the summary which includes coefficients and statistical information. Adjust the order and seasonal_order parameters as needed based on your analysis and grid search results."
      ],
      "metadata": {
        "id": "2-t1xN1vkPzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display diagnostic plots for the SARIMA model results\n",
        "results.plot_diagnostics(figsize=(16, 8))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F6GTGDCHRLuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate and plot one-step ahead forecast with confidence interval\n",
        "pred = results.get_prediction(start=pd.to_datetime('2003-01-06'), dynamic=False)\n",
        "pred_val = pred.conf_int()\n",
        "\n",
        "# Plot observed data and forecast\n",
        "ax = new_data['2002':].plot(label='observed')\n",
        "pred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7, figsize=(14, 7))\n",
        "\n",
        "# Fill the area between the upper and lower confidence bounds\n",
        "ax.fill_between(pred_val.index,\n",
        "                pred_val.iloc[:, 0],\n",
        "                pred_val.iloc[:, 1], color='k', alpha=.2)\n",
        "\n",
        "# Set axis labels and display legend\n",
        "ax.set_xlabel('ORDERDATE')\n",
        "ax.set_ylabel('SALES')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "om486gq9RTNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Mean Squared Error (MSE) and Root Mean Squared Error (RMSE)\n",
        "y_forecasted = pred.predicted_mean\n",
        "y_truth = new_data['SALES']\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "mse = mean_squared_error(y_forecasted, y_truth)\n",
        "rmse = sqrt(mse)\n",
        "\n",
        "print('The Mean Squared Error of the forecasts is {}'.format(round(rmse, 2)))"
      ],
      "metadata": {
        "id": "Q16oRhaFRXjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Out of sample forecast:\n",
        "\n",
        "To forecast sales values after some time period of the given data. In our case, we have to forecast sales with time period of 7 days."
      ],
      "metadata": {
        "id": "HXT0UryCRa2k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sales Foecast for Next 7 Days"
      ],
      "metadata": {
        "id": "ZKcXdVGuRfbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Forecast future values for the next 7 steps\n",
        "forecast = results.forecast(steps=7)\n",
        "print(forecast.astype('float'))"
      ],
      "metadata": {
        "id": "VsGjFTlpRyo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the forecast to a DataFrame and save it to a CSV file\n",
        "forecast = forecast.astype('float')\n",
        "forecast_df = forecast.to_frame()\n",
        "forecast_df.reset_index(level=0, inplace=True)\n",
        "forecast_df.columns = ['Prediction Date', 'Predicted Sales']\n",
        "prediction = pd.DataFrame(forecast_df).to_csv('prediction.csv', index=False)"
      ],
      "metadata": {
        "id": "oboDjsTUR3OE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the predicted data from the 'prediction.csv' file into a DataFrame\n",
        "df = pd.read_csv('./prediction.csv')\n",
        "\n",
        "# Plot the data\n",
        "df.plot()"
      ],
      "metadata": {
        "id": "AIe3s0fUR_QF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance Optimization"
      ],
      "metadata": {
        "id": "CP2R-xp9jaG9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimize Spark jobs for better performance by tuning parameters such as the number of partitions, memory allocation, and caching strategies."
      ],
      "metadata": {
        "id": "wRTysOL3VScE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure Spark properties for better performance\n",
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "try:\n",
        "    # Initialize Spark configuration\n",
        "    conf = SparkConf().setMaster(\"yarn\").setAppName(\"MySparkApp\")\n",
        "    sc = SparkContext(conf=conf)\n",
        "    spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "    # Set the number of partitions based on the cluster configuration\n",
        "    num_partitions = sc._conf.get(\"spark.sql.shuffle.partitions\")\n",
        "\n",
        "    # Cache frequently used dataframes to improve performance\n",
        "    df.cache()\n",
        "\n",
        "    # Use efficient data structures like Parquet for storing and reading data\n",
        "    df.write.parquet(\"hdfs:///path/to/data.parquet\")\n",
        "    df = spark.read.parquet(\"hdfs:///path/to/data.parquet\")\n",
        "\n",
        "    # Use broadcast variables to efficiently distribute large datasets\n",
        "    broadcast_var = sc.broadcast(some_large_dataset)\n",
        "\n",
        "    # Optimize joins by using appropriate join strategies\n",
        "    df1.join(df2, on=\"key\", how=\"inner\")\n",
        "\n",
        "    # Tune memory allocation and garbage collection settings\n",
        "    spark.conf.set(\"spark.memory.fraction\", 0.6)\n",
        "    spark.conf.set(\"spark.memory.storageFraction\", 0.5)\n",
        "    spark.conf.set(\"spark.shuffle.memoryFraction\", 0.3)\n",
        "    spark.conf.set(\"spark.cleaner.referenceTracking.cleaners\", \"org.apache.spark.storage.MemoryStoreCleaner\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "    # Handle the error or log it as needed\n",
        "# finally:\n",
        "    # Close SparkContext to release resources\n",
        "    # if 'sc' in locals() and sc is not None:\n",
        "    #     sc.stop()\n",
        "# Use efficient algorithms and libraries for specific tasks\n",
        "# (e.g., MLlib for machine learning, GraphFrames for graph processing)\n"
      ],
      "metadata": {
        "id": "RW4quTfXTYb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explore techniques like broadcast variables or accumulator variables to improve efficiency."
      ],
      "metadata": {
        "id": "k-3rbYSTVQJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use Spark's accumulator variables to efficiently aggregate data across multiple nodes\n",
        "acc = sc.accumulator(0)\n",
        "def f(x):\n",
        "    global acc\n",
        "    acc += x\n",
        "sc.parallelize(range(100)).foreach(f)\n",
        "print(acc.value)\n"
      ],
      "metadata": {
        "id": "FSIacdaLU-RH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reporting and Visualization"
      ],
      "metadata": {
        "id": "Ehn3WYjgVanu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Monthly and Weekly Revenue Trend"
      ],
      "metadata": {
        "id": "YsyjaBp-Vq4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first few rows of the DataFrame\n",
        "df_org.head()"
      ],
      "metadata": {
        "id": "MAFuDWAxWYxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Revenue by month\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "order=['Jan','Feb', 'Mar','Apr','May','Jun','Jul','Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
        "monthly_revenue=df_org.groupby(['MONTH_ID', 'YEAR_ID'])[['SALES']].sum().reset_index()\n",
        "ax=sns.barplot(data=monthly_revenue, x='MONTH_ID', y='SALES', hue='YEAR_ID', palette='pastel')\n",
        "plt.title('monthly_revenue')\n",
        "ax.set_xlabel('month')\n",
        "ax.set_xticklabels(order)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zKmLfE7OV2ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Revenue by week\n",
        "# Create new column weekday/month/year+quarter\n",
        "df_org['ORDERDATE']=pd.to_datetime(df_org['ORDERDATE'])\n",
        "df_org['ORDERDATE'].info()\n",
        "df_org['WEEK_DAY']=df_org['ORDERDATE'].dt.strftime('%a')\n",
        "\n",
        "order=['Mon','Tue','Wed', 'Thu','Fri','Sat', 'Sun']\n",
        "weekly_revenue=df_org.groupby(['WEEK_DAY', 'YEAR_ID'])[['SALES']].sum().reset_index()\n",
        "ax=sns.barplot(data=weekly_revenue, x='WEEK_DAY', y='SALES', hue='YEAR_ID', palette='pastel')\n",
        "plt.title('weekly_revenue')\n",
        "ax.set_xlabel('WEEK_DAY')\n",
        "ax.set_xticklabels(order)\n",
        "weekly_revenue"
      ],
      "metadata": {
        "id": "TDVz0GcvV-wb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observations:</br>\n",
        "1/ There are only 5 months data in 2005, so it's not intact, we should consider while looking at yearly revenue </br>\n",
        "2/ From monthly perspective, the second half year (Jul-Dec) has a speedy growth in sales then the first half, it might be a purchasing season for this industry </br>\n",
        "3/ From a weekly perspective, Thursday has the lowest buy rate thorughout a week, while closer to weekend the stronger purchasing power is than weekday"
      ],
      "metadata": {
        "id": "UIOq8Q8HW9PO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unit Price Distribution"
      ],
      "metadata": {
        "id": "uTAbIabnXZWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Scatter plot with seaborn\n",
        "sns.scatterplot(data=df_org, x='PRICEEACH', y='MSRP', hue='DEALSIZE')\n",
        "\n",
        "# Add a vertical line at x=26\n",
        "sns.lineplot(x=(26, 26), y=(100, 100), linestyle='--', color='r')\n",
        "\n",
        "# Set plot title and axis labels\n",
        "plt.title(\"Unit Price x MSRP Distribution\")\n",
        "plt.xlabel('Unit Price')\n",
        "plt.ylabel(\"MSRP\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eASC8wuqXSwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Scatter plot with seaborn\n",
        "sns.scatterplot(data=df_org, x='PRICEEACH', y='QUANTITYORDERED', hue='YEAR_ID')\n",
        "\n",
        "# Set plot title and axis labels\n",
        "plt.title(\"Unit Price x Q'ty Yearly Distribution\")\n",
        "plt.xlabel('Unit Price')\n",
        "plt.ylabel(\"Q'ty\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1DcnYBEZXu-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observations:</br>\n",
        "1/ Minimum unit price is 26 and maximum is 100 throughout 2003-2005 </br>\n",
        "2/ It's interetsing to see from \"Unit Price x Q'ty Yearly Distribution\" that 2003 and 2004 order q'ty are pretty stable around 20-50 </br>\n",
        "3/ While 2005 some order q'ty jump out of 20-50pcs, a little bit less and more, which we cannot see in the last 2 years </br>\n",
        "4/ From plot\"Unit Price Among Different Products\", Vintage car usually sells at lower price ($30-$50), plane and ships usually sells at higher price($60-$90), and Train $45-70,</br>\n",
        "5/ Motorcycle and classic cars have relative same proportion at each price range"
      ],
      "metadata": {
        "id": "rLnP8T3YX0bG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusions\n",
        "1/ 2005 Sales data only have 5 months, it need to be considered while checking yearly revenue. </br>\n",
        "2/ Top 10 countries supply over 80-95% revenue </br>\n",
        "3/ The most popular product line is classic cars, and the biggest market is USA </br>\n",
        "4/ In 2003&2004 most order q'ty are around 20-50pcs, in 2005 we can see some orders q'ty are more than that section </br>\n",
        "5/ Deal size distribution : Medium > Small > Large </br>\n",
        "6/ Vintage car usually sells at lower price ($30-$50), Train $45-70, Plane and ships usually sells at higher price($60-$90); Motorcycle and classic cars have relative same proportion at each price range 7.Second-half year (Jul-Dec) has a speedy growth in sales then the first half, it might be a purchasing season for this industry </br>\n",
        "7/ From a weekly perspective, Thursday has the lowest buy rate thorughout a week, while closer to weekend stronger the purchasing power is than weekday </br>\n",
        "8/ There a few msrp(manufactured suggest resell price) lower than unit price, usually it stands for distrbutor is doing a money-losing business, it deserves further investigation </br>\n",
        "9/ Among orders that are not shipped, one 2004 order is on hold </br>\n",
        "10/ There are some data missing and their missing rate below:\n",
        "</br> addressline2 missing: 89.33%/ state missing: 52.66%/ postalcode missing: 2.69%/ territory missing: 38.06% </br>\n"
      ],
      "metadata": {
        "id": "rflxuALmYPYm"
      }
    }
  ]
}